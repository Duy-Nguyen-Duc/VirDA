{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd56c25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.amp import GradScaler, autocast        \n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR  \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from yacs.config import CfgNode as CN\n",
    "\n",
    "from data import make_dataset, transform_map\n",
    "from eval import evaluate\n",
    "from model import UModel\n",
    "from torch_utils import ema_update, freeze_layers\n",
    "from utils import setup, clean_exp_savedir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99706095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights.\n"
     ]
    }
   ],
   "source": [
    "cfg = CN(new_allowed=True)\n",
    "cfg.merge_from_file(\"config.yaml\")\n",
    "\n",
    "source_train_loader, target_train_loader, source_test_loader, target_test_loader = (\n",
    "    make_dataset(\n",
    "        source_dataset=cfg.dataset.source,\n",
    "        target_dataset=cfg.dataset.target,\n",
    "        imgsize=cfg.img_size,\n",
    "        train_bs=cfg.domain_adapt.train_bs,\n",
    "        eval_bs=cfg.domain_adapt.eval_bs,\n",
    "        num_workers=cfg.domain_adapt.num_workers,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "model = UModel(\n",
    "    backbone=cfg.model.backbone.type,\n",
    "    hidden_dim=cfg.model.backbone.hidden_dim,\n",
    "    out_dim=cfg.dataset.num_classes,\n",
    "    imgsize=cfg.img_size,\n",
    "    freeze_backbone=cfg.model.backbone.freeze,\n",
    ")\n",
    "\n",
    "device = torch.device(cfg.device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7910cfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EigenCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.activations = None\n",
    "        self.hook_handle = None\n",
    "        \n",
    "    def save_activation(self, module, input, output):\n",
    "        self.activations = output\n",
    "        \n",
    "    def register_hook(self):\n",
    "        if self.hook_handle is None:\n",
    "            self.hook_handle = self.target_layer.register_forward_hook(self.save_activation)\n",
    "    \n",
    "    def remove_hook(self):\n",
    "        if self.hook_handle is not None:\n",
    "            self.hook_handle.remove()\n",
    "            self.hook_handle = None\n",
    "    \n",
    "    def __call__(self, input_tensor, branch=\"tch\"):\n",
    "        self.activations = None\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _ = self.model(input_tensor, branch=branch)\n",
    "        \n",
    "        if self.activations is None:\n",
    "            raise RuntimeError(\"No activations captured. Check target layer.\")\n",
    "        \n",
    "        activations = self.activations\n",
    "        B = activations.shape[0]\n",
    "        \n",
    "        if activations.shape[1] > 1:\n",
    "            activations = activations[:, 1:, :]\n",
    "        \n",
    "        original_dtype = activations.dtype\n",
    "        if activations.dtype == torch.float16:\n",
    "            activations = activations.float()\n",
    "        \n",
    "        mean = activations.mean(dim=2, keepdim=True)\n",
    "        centered = activations - mean\n",
    "        \n",
    "        cov = torch.bmm(centered, centered.transpose(1, 2))\n",
    "        \n",
    "        v = torch.randn(B, cov.shape[1], 1, device=cov.device, dtype=cov.dtype)\n",
    "        \n",
    "        for _ in range(10):\n",
    "            v = torch.bmm(cov, v)\n",
    "            v = v / (torch.norm(v, dim=1, keepdim=True) + 1e-10)\n",
    "        \n",
    "        cam = v.squeeze(-1)\n",
    "        \n",
    "        num_patches_side = int(cam.shape[1] ** 0.5)\n",
    "        cam = cam.view(B, num_patches_side, num_patches_side)\n",
    "        \n",
    "        cam_min = cam.view(B, -1).min(dim=1, keepdim=True)[0].view(B, 1, 1)\n",
    "        cam_max = cam.view(B, -1).max(dim=1, keepdim=True)[0].view(B, 1, 1)\n",
    "        cam = (cam - cam_min) / (cam_max - cam_min + 1e-10)\n",
    "        \n",
    "        if original_dtype == torch.float16:\n",
    "            cam = cam.half()\n",
    "\n",
    "        return cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e433ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = next(iter(source_train_loader))\n",
    "target_data = next(iter(target_train_loader))\n",
    "_, src_k_data, src_labels, _ = source_data\n",
    "tgt_q_data, tgt_k_data, _, affine_params = target_data\n",
    "\n",
    "src_img = src_k_data.to(device)\n",
    "src_labels = src_labels.to(device)\n",
    "tgt_k_img = tgt_k_data.to(device)  # strong\n",
    "tgt_q_img = tgt_q_data.to(device)  # weak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2227ed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = EigenCAM(model, model.backbone.transformer.blocks[-1].norm2)\n",
    "cam.register_hook()\n",
    "sal_map = cam(tgt_q_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc62e337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_map(salience_map, affine_params, transform_params=[0.0, 1.0], imgsize=224):\n",
    "    bg_w, fg_w = transform_params\n",
    "    device = salience_map.device if torch.is_tensor(salience_map) else 'cpu'\n",
    "    \n",
    "    if not torch.is_tensor(salience_map):\n",
    "        salience_map = torch.from_numpy(salience_map).to(device)\n",
    "    \n",
    "    salience_map = bg_w * (1 - salience_map) + fg_w * salience_map # [B, H, W]\n",
    "    salience_map = salience_map.unsqueeze(1) # [B, 1, H, W]\n",
    "            \n",
    "    angles = torch.as_tensor(affine_params['angle'], dtype=torch.float32, device=device).view(-1)\n",
    "    scales = torch.as_tensor(affine_params['scale'], dtype=torch.float32, device=device).view(-1)\n",
    "    translates = torch.stack(affine_params['translate'], dim=1).to(dtype=torch.float32)\n",
    "    shears = torch.stack(affine_params['shear'], dim=1).to(dtype=torch.float32)\n",
    "    \n",
    "    affine_matrices = _get_affine_matrix_vectorized(\n",
    "        angles, translates, scales, shears, salience_map.shape[-2:], device\n",
    "    )\n",
    "    \n",
    "    grid = F.affine_grid(\n",
    "        affine_matrices, \n",
    "        salience_map.shape, \n",
    "        align_corners=False\n",
    "    )\n",
    "    \n",
    "    transformed_maps = F.grid_sample(\n",
    "        salience_map, \n",
    "        grid, \n",
    "        mode='bilinear', \n",
    "        padding_mode='zeros',\n",
    "        align_corners=False\n",
    "    )\n",
    "    \n",
    "    if transformed_maps.shape[-1] != imgsize or transformed_maps.shape[-2] != imgsize:\n",
    "        transformed_maps = F.interpolate(\n",
    "            transformed_maps, \n",
    "            size=(imgsize, imgsize), \n",
    "            mode='bilinear', \n",
    "            align_corners=False\n",
    "        )\n",
    "    \n",
    "    return transformed_maps\n",
    "\n",
    "def _get_affine_matrix_vectorized(angles, translates, scales, shears, img_size, device):\n",
    "    angles = torch.as_tensor(angles, dtype=torch.float32, device=device)\n",
    "    translates = torch.as_tensor(translates, dtype=torch.float32, device=device)\n",
    "    scales = torch.as_tensor(scales, dtype=torch.float32, device=device)\n",
    "    shears = torch.as_tensor(shears, dtype=torch.float32, device=device)\n",
    "    \n",
    "    batch_size = angles.shape[0]\n",
    "\n",
    "    angle_rad = torch.deg2rad(angles)\n",
    "    shear_x_rad = torch.deg2rad(shears[:, 0])\n",
    "    shear_y_rad = torch.deg2rad(shears[:, 1])\n",
    "\n",
    "    height, width = img_size\n",
    "    center = torch.tensor([width / 2.0, height / 2.0], device=device).view(1, 2)\n",
    "\n",
    "    cos_a = torch.cos(angle_rad)\n",
    "    sin_a = torch.sin(angle_rad)\n",
    "    rotation_matrix = torch.eye(3, device=device).unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "    rotation_matrix[:, 0, 0] = cos_a\n",
    "    rotation_matrix[:, 0, 1] = -sin_a\n",
    "    rotation_matrix[:, 1, 0] = sin_a\n",
    "    rotation_matrix[:, 1, 1] = cos_a\n",
    "\n",
    "    scale_matrix = torch.eye(3, device=device).unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "    scale_matrix[:, 0, 0] = scales\n",
    "    scale_matrix[:, 1, 1] = scales\n",
    "\n",
    "    tan_sx = torch.tan(shear_x_rad)\n",
    "    tan_sy = torch.tan(shear_y_rad)\n",
    "    shear_matrix = torch.eye(3, device=device).unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "    shear_matrix[:, 0, 1] = -tan_sx\n",
    "    shear_matrix[:, 1, 0] = -tan_sy\n",
    "    \n",
    "    matrix = torch.bmm(shear_matrix, rotation_matrix)\n",
    "    matrix = torch.bmm(scale_matrix, matrix)\n",
    "    \n",
    "    c = translates[:, 0] + center[:, 0] - matrix[:, 0, 0] * center[:, 0] - matrix[:, 0, 1] * center[:, 1]\n",
    "    f = translates[:, 1] + center[:, 1] - matrix[:, 1, 0] * center[:, 0] - matrix[:, 1, 1] * center[:, 1]\n",
    "\n",
    "    final_matrix = torch.zeros(batch_size, 2, 3, device=device)\n",
    "    final_matrix[:, :, :2] = matrix[:, :2, :2]\n",
    "    final_matrix[:, 0, 2] = c\n",
    "    final_matrix[:, 1, 2] = f\n",
    "\n",
    "    return final_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36aae461",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_map = transform_map(sal_map, affine_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a0792d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 224, 224])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_map.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
